{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WELCOME TO PERFECT-PULLUP**\n",
    "\n",
    "The goal of this project is to estimate if a pull-up, taken from video, is a full range-of-motion, or \"perfect,\" pull-up. This project utilizes a pre-trained deep learning human pose estimation model to extract key points and assess the form of the pull-ups exercise. \n",
    "\n",
    "The pre-trained model used is the **Caffe** model.\n",
    "\n",
    "**Caffe = Convolutional Architecture for Fast Feature Embedding.**\n",
    "Caffe is an open-source deep learning framework developed by the Berkeley Vision and Learning Center (BVLC).\n",
    "\n",
    "**Layers:**\n",
    "The network consists of multiple convolutional layers (Convolution), followed by ReLU activation layers (ReLU) and pooling layers (Pooling).\n",
    "The convolutional layers perform feature extraction, while the ReLU layers introduce non-linearity.\n",
    "The pooling layers downsample the feature maps, reducing their spatial dimensions.\n",
    "*The layers are described in prototxt file in the models folder.\n",
    "\n",
    "**Hyper-Parameters:**\n",
    "In the parameters of the layers the convolutions are inside, a learning rate multipliers of 4.0 and 8.0 are used with decay multipliers of 1.0 and 0 respectively.\n",
    "\n",
    "**Architecture:**\n",
    "The architecture follows a Convolutional Pose Machine (CPM) approach.\n",
    "It consists of multiple stages, each containing several convolutional layers followed by ReLU activations.\n",
    "Each stage refines the pose estimation progressively, typically starting with low-resolution feature maps and gradually refining them to higher resolutions.\n",
    "\n",
    "**Output:**\n",
    "The layers output feature maps that correspond to keypoints during the human pose estimation task.\n",
    "Example heatmap of all of the 18 keypoints from the model:\n",
    "<img src=\"media/heatmaps.png\" alt=\"Screenshot\" width=\"1180\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Literature Review:**\n",
    "The first piece of literature is \"Caffe: Convolutional Architecture for Fast Feature Embedding,\" a report by Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama and Trevor Darrell at UC Berkeley.\n",
    "\n",
    "The model I used is a variant of the Caffe model. The Caffe model \"can also be used to extract semantic features from images using a pre-trained network\" (pg. 678, para. 1).\n",
    "\n",
    "A layer in the CNN-based Caffe model takes in blob as input and yields one as an output. \"Layers have two key responsibilities for the\n",
    "operation of the network as a whole: a forward pass that takes the inputs and produces the outputs, and a backward pass that takes the gradient with respect to the output, and computes the gradients with respect to the parameters and to the input\" (pg. 677, para. 3).\n",
    "\n",
    "Using a pre-trained model means this is done already, and I only need to use a forward pass after setting a blob from the frame as input to get the desired outputs.\n",
    "\n",
    "Below is an example of extracting a singular feature (left shoulder) when detecting keypoints in an image.\n",
    "\n",
    "<img src=\"media/feature.png\" alt=\"Screenshot\" width=\"200\"/>\n",
    "<img src=\"media/with-feature.png\" alt=\"Screenshot\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model I chose has max pooling layers. The report \"Pooling Methods in Deep Neural Networks, a Review,\" by Hossein Gholamalinezhad and Hossein Khosravi, says \"In max pooling, the maximum activation is selected from each pooling region\" (Section 2.5). \n",
    "\n",
    "Figure from section 2.2 of the report:\n",
    "\n",
    "<img src=\"media/max_pooling.png\" alt=\"Screenshot\" width=\"400\"/>\n",
    "\n",
    "In keypoint detection (or other topics), max pooling is a method for downsampling feature maps. By selecting the maximum activation within each pooling region, max pooling retains the most prominent features while discarding less relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State of the pull-up\n",
    "class PullUpState(Enum):\n",
    "    BOTTOM = 0\n",
    "    TOP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the frame for better processing\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.resize(frame, None, fx=0.5, fy=0.5)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keypoints(frame, output, keypoints_mapping, threshold):\n",
    "    H, W = frame.shape[:2]\n",
    "    \n",
    "    keypoints_of_interest = [\"LShoulder\", \"RShoulder\", \"LElbow\", \"RElbow\", \"LWrist\", \"RWrist\"]\n",
    "    \n",
    "    for keypoint in keypoints_of_interest:\n",
    "        # Get the index of the keypoint in the keypoints mapping\n",
    "        index = keypoints_mapping.index(keypoint)\n",
    "        \n",
    "        # Get the probability map for the keypoint\n",
    "        prob_map = output[0, index, :, :]\n",
    "        prob_map = cv2.resize(prob_map, (W, H))\n",
    "        \n",
    "        # Find the maximum confidence value and its location in the probability map\n",
    "        _, confidence, _, point = cv2.minMaxLoc(prob_map)\n",
    "        \n",
    "        # Check if the confidence value exceeds the threshold\n",
    "        if confidence > threshold:\n",
    "            cv2.circle(frame, point, 5, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "            cv2.putText(frame, f\"{keypoint}: {confidence:.2f}\", (point[0] + 6, point[1] + 6),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    # Return the frame with keypoints drawn on it\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, output, keypoints_pairs, keypoints_mapping, threshold):\n",
    "    H, W = frame.shape[:2]\n",
    "    \n",
    "    for pair in keypoints_pairs:\n",
    "        # Check if either of the keypoints in the pair is one of the keypoints of interest\n",
    "        if pair[0] in [keypoints_mapping.index(\"LShoulder\"), keypoints_mapping.index(\"RShoulder\"),\n",
    "                       keypoints_mapping.index(\"LElbow\"), keypoints_mapping.index(\"RElbow\"),\n",
    "                       keypoints_mapping.index(\"LWrist\"), keypoints_mapping.index(\"RWrist\")]:\n",
    "            # Get the indices of the keypoints in the pair\n",
    "            index1, index2 = pair\n",
    "            \n",
    "            # Get the maximum confidence scores for both keypoints in the pair\n",
    "            confidence1 = output[0, index1, :, :].max()\n",
    "            confidence2 = output[0, index2, :, :].max()\n",
    "            \n",
    "            # Check if both keypoints have confidence scores above the threshold\n",
    "            if confidence1 > threshold and confidence2 > threshold:\n",
    "                # Get the probability maps for both keypoints\n",
    "                prob_map1 = output[0, index1, :, :]\n",
    "                prob_map2 = output[0, index2, :, :]\n",
    "                \n",
    "                # Find the location of the maximum confidence points for both keypoints\n",
    "                _, _, _, point1 = cv2.minMaxLoc(prob_map1)\n",
    "                _, _, _, point2 = cv2.minMaxLoc(prob_map2)\n",
    "                \n",
    "                # Calculate the pixel coordinates of the keypoints in the original frame\n",
    "                x1, y1 = int(W * point1[0] / prob_map1.shape[1]), int(H * point1[1] / prob_map1.shape[0])\n",
    "                x2, y2 = int(W * point2[0] / prob_map2.shape[1]), int(H * point2[1] / prob_map2.shape[0])\n",
    "                \n",
    "                # Draw a line connecting the keypoints on the frame\n",
    "                cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "    \n",
    "    # Return the frame with connections drawn on it\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pull_ups(output, SHOULDER_INDEX, ELBOW_INDEX, WRIST_INDEX, threshold, ANGLE_THRESHOLD_BOTTOM, ANGLE_THRESHOLD_TOP, current_state, repetition_count, prev_angle):\n",
    "    # Extract coordinates of shoulder, elbow, and wrist points from the output\n",
    "    shoulder_point = output[0, SHOULDER_INDEX, :, :]\n",
    "    elbow_point = output[0, ELBOW_INDEX, :, :]\n",
    "    wrist_point = output[0, WRIST_INDEX, :, :]\n",
    "\n",
    "    # Check if all keypoints have confidence scores above the threshold\n",
    "    if all(output[0, index, :, :].max() > threshold for index in [SHOULDER_INDEX, ELBOW_INDEX, WRIST_INDEX]):\n",
    "        # Find the location of maximum confidence for each keypoint\n",
    "        _, _, _, shoulder_max_loc = cv2.minMaxLoc(shoulder_point)\n",
    "        _, _, _, elbow_max_loc = cv2.minMaxLoc(elbow_point)\n",
    "        _, _, _, wrist_max_loc = cv2.minMaxLoc(wrist_point)\n",
    "\n",
    "        # Convert locations to numpy arrays\n",
    "        shoulder_coords = np.array(shoulder_max_loc)\n",
    "        elbow_coords = np.array(elbow_max_loc)\n",
    "        wrist_coords = np.array(wrist_max_loc)\n",
    "\n",
    "        # Calculate vectors representing upper arm and forearm\n",
    "        upper_arm_vector = shoulder_coords - elbow_coords\n",
    "        forearm_vector = wrist_coords - elbow_coords\n",
    "        \n",
    "        # Calculate dot product and magnitudes of vectors\n",
    "        dot_product = np.dot(upper_arm_vector, forearm_vector)\n",
    "        upper_arm_magnitude = np.linalg.norm(upper_arm_vector)\n",
    "        forearm_magnitude = np.linalg.norm(forearm_vector)\n",
    "\n",
    "        # Calculate angle between upper arm and forearm vectors\n",
    "        angle_radians = np.arccos(dot_product / (upper_arm_magnitude * forearm_magnitude))\n",
    "        angle_degrees = np.degrees(angle_radians)\n",
    "\n",
    "        # Check for state transitions\n",
    "        prev_angle = angle_degrees\n",
    "        \n",
    "        if current_state == PullUpState.BOTTOM:\n",
    "            if angle_degrees < ANGLE_THRESHOLD_TOP:\n",
    "                print(\"Entered TOP state\")\n",
    "                current_state = PullUpState.TOP\n",
    "        elif current_state == PullUpState.TOP:\n",
    "            if angle_degrees > ANGLE_THRESHOLD_BOTTOM:\n",
    "                print(\"Entered BOTTOM state\")\n",
    "                repetition_count += 1\n",
    "                current_state = PullUpState.BOTTOM\n",
    "\n",
    "    # Return updated state, repetition count, and previous angle\n",
    "    return current_state, repetition_count, prev_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_heatmaps(output, keypoints_mapping):\n",
    "    num_keypoints = len(keypoints_mapping)\n",
    "    heatmaps = []\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for index in range(num_keypoints):\n",
    "        # Get the probability map for the keypoint\n",
    "        prob_map = output[0, index, :, :]\n",
    "\n",
    "        # Normalize the probability map to the range [0, 255]\n",
    "        prob_map = cv2.normalize(prob_map, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "        # Apply a color map for better visualization\n",
    "        heatmap_colored = cv2.applyColorMap(prob_map, cv2.COLORMAP_JET)\n",
    "        \n",
    "        # Append the colored heatmap to the list\n",
    "        heatmaps.append(heatmap_colored)\n",
    "\n",
    "    # Concatenate all the heatmaps horizontally for visualization\n",
    "    heatmaps_combined = np.hstack(heatmaps)\n",
    "\n",
    "    return heatmaps_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example heatmap of all of the 18 keypoints from the model\n",
    "<img src=\"media/heatmaps.png\" alt=\"Screenshot\" width=\"1180\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Caffe deep neural network for pose estimation\n",
    "net = cv2.dnn.readNet('models/pose_iter_440000.caffemodel', 'models/pose_deploy_linevec.prototxt')\n",
    "# caffemodel file stores the weights of the trained model\n",
    "# prototxt file stores the architecture of the neural network, defining the layers and their connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video file\n",
    "#cap = cv2.VideoCapture(1)\n",
    "cap = cv2.VideoCapture('media/cam.mp4') \n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the confidence threshold for keypoint detection\n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keypoints names and the index pairs of keypoints that should be connected\n",
    "keypoints_mapping = [\n",
    "    \"Nose\", \"Neck\", \"RShoulder\", \"RElbow\", \"RWrist\", \"LShoulder\", \"LElbow\", \"LWrist\",\n",
    "    \"RHip\", \"RKnee\", \"RAnkle\", \"LHip\", \"LKnee\", \"LAnkle\", \"REye\", \"LEye\", \"REar\", \"LEar\"\n",
    "]\n",
    "\n",
    "keypoints_pairs = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7),\n",
    "    (1, 14), (14, 16), (14, 15), (1, 8), (8, 9), (9, 10),\n",
    "    (11, 12), (12, 13)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting state\n",
    "current_state = PullUpState.BOTTOM\n",
    "repetition_count = 0\n",
    "\n",
    "prev_angle = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indices of key points for shoulders, elbows, and wrists\n",
    "SHOULDER_INDEX = keypoints_mapping.index(\"LShoulder\")\n",
    "ELBOW_INDEX = keypoints_mapping.index(\"LElbow\")\n",
    "WRIST_INDEX = keypoints_mapping.index(\"LWrist\")\n",
    "\n",
    "# Threshold for arm extension and flexion angle (in degrees)\n",
    "ANGLE_THRESHOLD_BOTTOM = 140  # Can adjust\n",
    "ANGLE_THRESHOLD_TOP = 59  # Can adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered TOP state\n",
      "Entered BOTTOM state\n",
      "Entered TOP state\n",
      "Entered BOTTOM state\n",
      "Entered TOP state\n",
      "Entered BOTTOM state\n",
      "Error: Failed to capture frame.\n"
     ]
    }
   ],
   "source": [
    "# Define frame counter to skip frames and improve speed\n",
    "frame_counter = 0\n",
    "\n",
    "# Start a loop to continuously capture frames\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    frame_counter += 1\n",
    "\n",
    "    # Process every 6th frame\n",
    "    if frame_counter % 6 != 0:\n",
    "        continue  \n",
    "\n",
    "    # Preprocess the frame\n",
    "    frame = preprocess_frame(frame)\n",
    "\n",
    "    # Generate blob from the frame, set it as the input to the network, and perform a forward pass\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (368, 368), (0, 0, 0), swapRB=False, crop=False)\n",
    "    net.setInput(blob)\n",
    "    output = net.forward() #Only a forward pass is needed as the network is already trained\n",
    "\n",
    "    # Visualize the heatmaps for each keypoint\n",
    "    heatmap = visualize_heatmaps(output, keypoints_mapping)\n",
    "\n",
    "    # Display the combined heatmaps\n",
    "    cv2.imshow(\"Heatmaps\", heatmap)\n",
    "\n",
    "    # Detect keypoints and draw them on the frame, then draw connections between them\n",
    "    frame = detect_keypoints(frame, output, keypoints_mapping, threshold)\n",
    "    frame = draw_connections(frame, output, keypoints_pairs, keypoints_mapping, threshold)\n",
    "\n",
    "    # Detect pull-ups and update state, repetition count, and previous angle\n",
    "    current_state, repetition_count, prev_angle = detect_pull_ups(output, SHOULDER_INDEX, ELBOW_INDEX, WRIST_INDEX, threshold, ANGLE_THRESHOLD_BOTTOM, ANGLE_THRESHOLD_TOP, current_state, repetition_count, prev_angle)\n",
    "\n",
    "    # Add info to frame\n",
    "    cv2.putText(frame, f\"Repetitions: {repetition_count}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"Angle: {prev_angle:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Pullup Pose', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
